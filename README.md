# Defense Against Evasion Attacks

The massive advancements in the field of machine learning have enabled it to be used in vital technologies such as self-driving cars, phishing page detection and anomaly detection. Machine learning classifiers used for these applications are required to be fast and precise as any faults in the predictions may be hazardous or may lead to leakage of sensitive information. However, in recent times, machine learning models deployed on the client-side, have been vulnerable to evasion attacks. An evasion attack happens when a model, during the testing phase, is fed with a carefully perturbed input that looks and feels exactly the same as its untampered copy to a human, but such adversarial examples usually completely throw off the classifier from working as intended.

Evasion attacks generally occur under the white box scenarios, in form of gradient-based attacks. In such situations, the attackers are well aware of the how the model works and has access to its gradients, and thus tries to mathematically optimize the attack by exploiting the constraints and loopholes present in the targeted classifier. The detailed knowledge about the model always allows the attackers to craft adversarial examples to fool the models. Evasion attacks are rarely also performed with the help of hard labels and surrogate models. These make evasion attack possible even with limited knowledge about the classifier, even in the grey, black box scenarios.

In this project, we simulate evasion attacks on image classification models and  analyze the effectiveness of the evasion attacks on the classifier and its impact on the performance of the model in terms of various test metrics. Further, we propose a defense mechanism that can be incorporated while training the model, which makes the model robust enough to classify even adversarial images provided while prediction phase into correct original classes.
