# DefenseAgainstEvasionAttacks

The massive advancements in the field of machine learning have enabled it to be used in vital technologies such as self-driving cars, phishing page detection and anomaly detection. Machine learning classifiers used for these applications are required to be fast and precise as any faults in the predictions may be hazardous or may lead to leakage of sensitive information. However, in recent times, machine learning models deployed on the client-side, have been vulnerable to evasion attacks. An evasion attack happens when a model, during the testing phase, is fed with a carefully perturbed input that looks and feels exactly the same as its untampered copy to a human, but such adversarial examples usually completely throw off the classifier from working as intended.

Evasion attacks generally occur under the white box scenarios, in form of gradient-based attacks. In such situations, the attackers are well aware of the how the model works and has access to its gradients, and thus tries to mathematically optimize the attack by exploiting the constraints and loopholes present in the targeted classifier. The detailed knowledge about the model always allows the attackers to craft adversarial examples to fool the models. Evasion attacks are rarely also performed with the help of hard labels and surrogate models. These make evasion attack possible even with limited knowledge about the classifier, even in the grey, black box scenarios.

In this project, we simulate evasion attacks on image classification model, under various assumptions on the amount of prior knowledge known to the attackers, about the system. The project would analyse the effectiveness and efficiency of the evasion attacks on the classifier and its impact on the performance of the model in terms of the confidence score, precision, robustness and F1-score. Further, we propose a similarity-based method that compares the new samples to be fed to the model against recently classified images, that would effectively detect evasion attacks. The system proposed includes a defense mechanism via which it retrains repeatedly, adjusting the weights, to classify even the adversarial examples correctly.